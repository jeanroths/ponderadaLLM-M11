# Desafio Extra - Rodar LLM no embarcado (Raspberry)

## Demonstração em Vídeo

O vídeo a seguir demonstra o **LLM Ollama rodando em um dispositivo embarcado**. O modelo é executado diretamente no hardware, sem necessidade de servidores externos, evidenciando a possibilidade de realizar processamento local de linguagem natural em sistemas de baixo consumo de energia.

